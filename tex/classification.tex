\documentclass[11pt]{article}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{graphicx}
\graphicspath{ {figures/} }

\usepackage{subcaption}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem*{claim}{Claim}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{example}{Example}[section]

\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\vect}[1]{\mathbf{#1}}

\title{Wide-Reach Classification}
\author{Vincenzo Liberatore}

\begin{document}

\maketitle

\section{Maximum Hyperplane Consistency}
Let $P$ be a finite multiset of integer {\em positive samples\/} and 
$N$ be a finite multiset of integer {\em negative samples\/} in $\R^n$.
Given a multiset $X$, we denote as $\nu_X(\vect{x})$ the multiplicity of $\vect{x}$ in $X$. 
Given a hyperplane $h = (\vect{w}, c)$ in $\R^n$ 
specified by a normal vector $\vect{w} = (w_1 ~ w_2 \dots ~ w_n)^T \in \Q^n$ and a bias $c \in \Q$,
define $C(P, h) = \{ \vect{x} \in P : \vect{x}^T \vect{w} > c \}$
to be the set of positive samples that are {\em consistent with the hyperplane\/} $(\vect{w}, c)$,
and similarly $C(N, h) = \{ \vect{x} \in P : \vect{x}^T \vect{w} < c \}$
to be the set of negative samples that are {\em consistent with the hyperplane\/} $(\vect{w}, c)$.
A hyperplane $h$ will be said to {\em satisfy a sample\/} 
$\vect{x} \in P$ ($\vect{x} \in N$)
if $\vect{x} \in C(P, h)$ ($\vect{x} \in C(N, h)$)
The {\em Maximum Hyperplane Consistency\/} problem is the problem in which,
given $P$ and $N$, we wish to find a hyperplane $h$
to maximize $|C(P, h)| + |C(N, h)|$, the number of samples 
that are consistent with the given hyperplane \cite{countingapprox}.


The special case in which $c$ is required to be $c = 0$ is the {\em Open Hemisphere\/} problem,
which is known to be NP-complete in general but solvable in polynomial time
when $n$ is constant \cite{densehemisphere}.
In the Open Hemisphere problem, it can be assumed without loss of generality that 
\begin{itemize} 
\item $N = \emptyset$ because 
any negative sample $\vect{x}$ can be replaced by the positive sample $-\vect{x}$
\item The zero vector $\vect{0}$ is not a sample because it cannot be satisfied by any hyperplane
\end{itemize}

The Maximum Hyperplane Consistency problem can be approximated within a factor of 2 \cite{countingapprox}.
The main idea of the algorithm is to process one dimension at a time 
by setting the component of $w$ along that dimension to maximize the number of satisfied vectors
whose only non-zero component is along that dimension.


Before discussing the algorithm, 
consider the simple problem in which we are given $m$ rationals $r_i$ and 
$s_i \in \{ \pm 1 \}$ coefficients ($i = 1, 2, \dots, m$),
and we wish to find a rational number $w$ that satisfies as many as possible of the inequalities
$x_i w > r_i$.
Let $r_i' = r_i / x_i$ and assume without loss of generality that 
$r_1' \leq r_2' \leq \dots \leq r_m'$.
Observe that the number of satisfied inequalities is constant in the 
intervals of $w$ corresponding to
$(-\infty, r_1'), (r_1', r_2'), \dots (r_m', \infty)$.
Then, the algorithm calculates the number of satisfied inequalities in each of these intervals 
as well as in the points $r_1', r_2', \dots, r_m'$, and pick the
value of $w$ that maximizes the number of satisfied inequalities. 

Returning to the Maximum Hyperplane Consistency problem,  
the algorithm starts with the special case of the zero sample $\vect{0}$, in which 
case it sets the bias $c$ to maximize the number of copies of $\vect{0}$ that are satisfied by
the assignment 
$$c = \begin{cases} 
1 & \text{if } \nu_N(\vect{0}) > \nu_P(\vect{0})\\
-1 & \text{if } \nu_P(\vect{0}) > \nu_C(\vect{0})\\
0 & \text{otherwise}
\end{cases}$$
At this point, the algorithm proceeds as a sequence of iterations to set the value of $\vect{w}$.
Assume that, at the beginning of the current iteration,
the algorithm has set the value of $c, w_{i+1}, \dots, w_n$.
Let $r(\vect{x}) = c - \sum_{j = i + 1}^n w_j x_j$ for any sample $\vect{x}$.
The algorithm checks if there is any sample that has a single non-zero component among its first $i$
(in other words, given a sample $\vect{x} = (x_1 ~ x_2 \dots x_i ~ x_{i+1} ~ x_n)^T$,
we search for a sample in which only one of $x_1, x_2, \dots , x_i$ is non zero).
If there is no such sample, the algorithm sets $w_i = 0$ and moves on to the next iteration.
Otherwise, assume without loss of generality that the non-zero component is the $i$th,
and let $S_i$ be the multiset of samples that are zero save in the $i$th component. 
The algorithm sets $w_i$ so as to maximize the number of samples in $S_i$ that are satisfied. 
Specifically, it considers the inequalities 
$x_i w_i > r(\vect{x})$ for $\vect{x} \in P$ and 
$x_i w_i < r(\vect{x})$ for $\vect{x} \in N$, and
chooses any value of $w_i$ that maximizes the number of such inequalities that are satisfied. 
It then proceeds to the next iteration.
To justify the correctness, note that a sample $\vect{x}$ is considered at only one iteration or,
in other words, that the algorithm partitions the samples into the $S_i$s.
Furthermore, the satisfied samples in $S_i$ are never less than the once that are not satisfied,
which means that the algorithm satisfies at least half of the samples.

A randomized algorithm for both problem is simple and achieves an approximation ratio of 2. 
Set $c = 0$ and choose a random vector $\vect{w}$ on the unit hypershpere, say, with the
method in \cite{rndvect}. Then, for any (positive) sample $\vect{x} \in P$,
$\Pr [ \vect{w}^T \vect{x} > 0 ] = 1 / 2$.

The Maximum Hyperplane Consistency problem can be generalized by associating
non-uniform weights to the samples. 
Furthermore, it is often required that 
$\vect{w}^T \vect{x} \geq c + \epsilon_P$ and 
$\vect{w}^T \vect{x} \leq c - \epsilon_N$
for some small constants $\epsilon_P, \epsilon_N > 0$ \cite{ClassificationOptSurvey}.


\section{Wide-Reach Classification}
Given a hyperplane $h$, the quantity $|C(P, h)|$ is defined as the {\em reach\/},
$|N - C(N,h)|$ as the number of {\em false positives\/},
and $|C(P,h)| / (|C(P,h)| + |N - C(N,h)|)$ as the {\em precision\/}.
The {\em Wide-Reach Classification\/} problem can be informally defined
as the problem of maximizing recall subject to the constraint that the precision
be larger than or equal to a precision threshold $\theta$.
However, such a combinatorial formulation runs against numerical instability problem. 


\bibliographystyle{plain}
\bibliography{classification}

\end{document}
  
