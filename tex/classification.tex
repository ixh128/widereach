\documentclass[11pt]{article}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{graphicx}
\graphicspath{ {figures/} }

\usepackage{subcaption}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem*{claim}{Claim}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{example}{Example}[section]

\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\sv}[1]{_{\vect{#1}}}

\title{Wide-Reach Classification}
\author{Vincenzo Liberatore}

\begin{document}

\maketitle

\section{Maximum Hyperplane Consistency}
Let $P$ be a finite multiset of integer {\em positive samples\/} and 
$N$ be a finite multiset of integer {\em negative samples\/} in $\R^n$.
Given a multiset $X$, we denote as $\nu_X(\vect{s})$ the multiplicity of $\vect{s}$ in $X$. 
Given a set $S$ of samples, a hyperplane $h = (\vect{w}, c)$ in $\R^n$ 
specified by a normal vector $\vect{w} = (w_1 ~ w_2 \dots ~ w_n)^T \in \Q^n$ and a bias $c \in \Q$,
define 
$$C^+(S, h) = \{ \vect{s} \in S : \vect{s}^T \vect{w} > c \}$$
and 
$$C^-(N, h) = \{ \vect{s} \in P : \vect{s}^T \vect{w} < c \}\;.$$
Define $C^+(P, h)$ ($C^-(N, h)$) be the set of positive 
(negative) samples that are 
{\em consistent with the hyperplane\/} $h$.
A hyperplane $h$ will be said to {\em satisfy a sample\/} 
$\vect{s} \in P$ ($\vect{s} \in N$)
if $\vect{s} \in C^+(P, h)$ ($\vect{s} \in C^-(N, h)$)
The {\em Maximum Hyperplane Consistency\/} problem is the problem in which,
given $P$ and $N$, we wish to find a hyperplane $h$
to maximize $|C(P, h)| + |C(N, h)|$, the number of samples 
that are consistent with the given hyperplane \cite{countingapprox}.

The special case in which $c$ is required to be $c = 0$ is the {\em Open Hemisphere\/} problem,
which is known to be NP-complete in general but solvable in polynomial time
when $n$ is constant \cite{densehemisphere}.
In the Open Hemisphere problem, it can be assumed without loss of generality that 
\begin{itemize} 
\item $N = \emptyset$ because 
any negative sample $\vect{s}$ can be replaced by the positive sample $-\vect{s}$
\item The zero vector $\vect{0}$ is not a sample because it cannot be satisfied by any hyperplane
\end{itemize}

The Maximum Hyperplane Consistency problem can be approximated within a factor of 2 \cite{countingapprox}.
The main idea of the algorithm is to process one dimension at a time 
by setting the component of $w$ along that dimension to maximize the number of satisfied vectors
whose only non-zero component is along that dimension.


Before discussing the algorithm, 
consider the simple problem in which we are given $m$ rationals $r_i$ and 
$s_i \in \{ \pm 1 \}$ coefficients ($i = 1, 2, \dots, m$),
and we wish to find a rational number $w$ that satisfies as many as possible of the inequalities
$x_i w > r_i$.
Let $r_i' = r_i / x_i$ and assume without loss of generality that 
$r_1' \leq r_2' \leq \dots \leq r_m'$.
Observe that the number of satisfied inequalities is constant in the 
intervals of $w$ corresponding to
$(-\infty, r_1'), (r_1', r_2'), \dots (r_m', \infty)$.
Then, the algorithm calculates the number of satisfied inequalities in each of these intervals 
as well as in the points $r_1', r_2', \dots, r_m'$, and pick the
value of $w$ that maximizes the number of satisfied inequalities. 

Returning to the Maximum Hyperplane Consistency problem,  
the algorithm starts with the special case of the zero sample $\vect{0}$, in which 
case it sets the bias $c$ to maximize the number of copies of $\vect{0}$ that are satisfied by
the assignment 
$$c = \begin{cases} 
1 & \text{if } \nu_N(\vect{0}) > \nu_P(\vect{0})\\
-1 & \text{if } \nu_P(\vect{0}) > \nu_C(\vect{0})\\
0 & \text{otherwise}
\end{cases}$$
At this point, the algorithm proceeds as a sequence of iterations to set the value of $\vect{w}$.
Assume that, at the beginning of the current iteration,
the algorithm has set the value of $c, w_{i+1}, \dots, w_n$.
Let $r(\vect{s}) = c - \sum_{j = i + 1}^n w_j x_j$ for any sample $\vect{s}$.
The algorithm checks if there is any sample that has a single non-zero component among its first $i$
(in other words, given a sample $\vect{s} = (x_1 ~ x_2 \dots x_i ~ x_{i+1} ~ x_n)^T$,
we search for a sample in which only one of $x_1, x_2, \dots , x_i$ is non zero).
If there is no such sample, the algorithm sets $w_i = 0$ and moves on to the next iteration.
Otherwise, assume without loss of generality that the non-zero component is the $i$th,
and let $S_i$ be the multiset of samples that are zero save in the $i$th component. 
The algorithm sets $w_i$ so as to maximize the number of samples in $S_i$ that are satisfied. 
Specifically, it considers the inequalities 
$x_i w_i > r(\vect{s})$ for $\vect{s} \in P$ and 
$x_i w_i < r(\vect{s})$ for $\vect{s} \in N$, and
chooses any value of $w_i$ that maximizes the number of such inequalities that are satisfied. 
It then proceeds to the next iteration.
To justify the correctness, note that a sample $\vect{s}$ is considered at only one iteration or,
in other words, that the algorithm partitions the samples into the $S_i$s.
Furthermore, the satisfied samples in $S_i$ are never less than the once that are not satisfied,
which means that the algorithm satisfies at least half of the samples.

A randomized algorithm for both problem is simple and achieves an approximation ratio of 2. 
Set $c = 0$ and choose a random vector $\vect{w}$ on the unit hypershpere, say, with the
method in \cite{rndvect}. Then, for any (positive) sample $\vect{s} \in P$,
$\Pr [ \vect{w}^T \vect{s} > 0 ] = 1 / 2$.

\section{Two-Group Misclassification}
The {\em Two-Group Misclassification\/} problem \cite{ClassificationOptSurvey}
is defined as the mixed-integer linear program: 
\begin{subequations}
\label{eq:twogroupmisclassification}
\begin{align}
\min\quad & 
\pi_P \sum_{\vect{s} \in P} \delta\sv{s} + 
\pi_N \sum_{\vect{s} \in N} \delta\sv{s} \\
\text{s.t.}\quad & 
\delta\sv{s} \geq c - \vect{s}^T \vect{w} + \epsilon_P & & (\vect{s} \in P) \\
& \delta\sv{s} \geq \vect{s}^T \vect{w} - c + \epsilon_N & & (\vect{s} \in N) \\
& \delta\sv{s} \in \{ 0, 1 \} & & (\vect{s} \in P \cup N) 
\end{align}
\end{subequations}
In this program, $\delta\sv{s}$ is a binary decision variable representing
the misclassification of $\vect{s}$ in that $\delta\sv{s}$ is 
0 if $\vect{s}$ is consistent with $h = (\vect{w}, c)$ and 
1 otherwise.
The program \eqref{eq:twogroupmisclassification} weights the misclassifications
by non-negative costs $\pi_P$ and $\pi_N$ 
for positive and negative samples respectively.
Furthermore, it is parametric in the two non-negative 
values of $\epsilon_P$ and $\epsilon_N$.

If $\epsilon_P = \epsilon_N = 0$, then a sample $\vect{s}$ that falls on the hyperplane $h$ (i.e., $\vect{s}^T \vect{w} = c$) are always considered to be correctly classified (i.e., $\delta\sv{s} = 0$ at optimality), regardless
of whether $\vect{s}$ is positive or negative.
Previous work has intensively investigated the numerical solution of 
\eqref{eq:twogroupmisclassification} \cite{ClassificationOptSurvey}, in which case 
$\delta\sv{s} = 0$ at optimality even when $\vect{s}^T \vect{w} \simeq c$
is within the rounding error. 
Thus, the program \eqref{eq:twogroupmisclassification} effectively
becomes the problem of finding
a hyperplane in whose neighborhood most samples fall, and can lead to 
misleading solutions. 
As a result, most of the literature sets at least one of $\epsilon_P$ or 
$\epsilon_N$ to be positive.
Henceforth, we will assume $\epsilon_P, \epsilon_N > 0$

Note that the Two-Group Misclassification problem always has the feasible
solutions $\vect{w} = 0$, $c = - \epsilon_P$ ($c = \epsilon_N$)
that classifies all positive (negative) samples correctly.
Additionally, the null solution $\vect{w} = \vect{0}$, $c = 0$ is feasible but 
it misclassifies all points and thus never optimal.


\section{Wide-Reach Classification}
Given a hyperplane $h$, the quantity $|C(P, h)|$ is defined as the {\em reach\/},
$|N - C(N,h)|$ as the number of {\em false positives\/},
and $|C(P,h)| / (|C(P,h)| + |N - C(N,h)|)$ as the {\em precision\/}.
The {\em Wide-Reach Classification\/} problem can be informally defined
as the problem of maximizing recall subject to the constraint that the precision
be larger than or equal to a precision threshold $\theta$.
However, such a combinatorial formulation runs against numerical instability problem. 


\bibliographystyle{plain}
\bibliography{classification}

\end{document}
  
